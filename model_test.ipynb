{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1688fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import glob\n",
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb486bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/Users/jaydenma/Documents/mathematical image analysis/mia final project/models/yolo11m_car1/weights/best.pt\"\n",
    "train_data_path = \"/Users/jaydenma/Documents/mathematical image analysis/mia final project/Cars Detection/train/images\"\n",
    "train_labels_path = \"/Users/jaydenma/Documents/mathematical image analysis/mia final project/Cars Detection/train/labels\"\n",
    "test_data_path = \"/Users/jaydenma/Documents/mathematical image analysis/mia final project/Cars Detection/test/images\"\n",
    "test_labels_path = \"/Users/jaydenma/Documents/mathematical image analysis/mia final project/Cars Detection/test/labels\"\n",
    "data_yaml_path = \"/Users/jaydenma/Documents/mathematical image analysis/mia final project/Cars Detection/data.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814868f8",
   "metadata": {},
   "source": [
    "## load YOLO models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c19ccd",
   "metadata": {},
   "source": [
    "### control model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e124b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YOLO model using the model_path variable that's already defined\n",
    "model = YOLO(model_path)\n",
    "print(f\"Model loaded from: {model_path}\")\n",
    "# print(f\"Model version: {model.model.version}\")\n",
    "print(f\"Model task: {model.task}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b330cd73",
   "metadata": {},
   "source": [
    "## get accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68e68e1",
   "metadata": {},
   "source": [
    "### Load class mapping and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e51ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load class names from the YAML file\n",
    "# data_yaml_path = '/Users/jaydenma/Documents/mathematical image analysis/mia final project/Cars Detection/data.yaml'\n",
    "with open(data_yaml_path, 'r') as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "\n",
    "class_names = data_config['names']\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "# Function to read ground truth labels\n",
    "def read_label_file(label_path):\n",
    "    \"\"\"Read YOLO format label file and return list of objects with class and bounding box\"\"\"\n",
    "    gt_objects = []\n",
    "    try:\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 5:  # class, x_center, y_center, width, height\n",
    "                    class_id = int(parts[0])\n",
    "                    gt_objects.append({\n",
    "                        'class_id': class_id,\n",
    "                        'class_name': class_names[class_id] if class_id < len(class_names) else f\"Unknown_{class_id}\"\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading label file {label_path}: {e}\")\n",
    "    return gt_objects\n",
    "\n",
    "# Function to compute class-based accuracy\n",
    "def compute_class_accuracy(predicted_classes, gt_classes):\n",
    "    \"\"\"Compute accuracy metrics per class\"\"\"\n",
    "    # Initialize counters for each class\n",
    "    class_metrics = {name: {'TP': 0, 'FP': 0, 'FN': 0} for name in class_names}\n",
    "    \n",
    "    # Convert lists to sets for easier comparison\n",
    "    pred_set = set(predicted_classes)\n",
    "    gt_set = set(gt_classes)\n",
    "    \n",
    "    # Count true positives, false positives, and false negatives\n",
    "    for cls in class_names:\n",
    "        if cls in pred_set and cls in gt_set:\n",
    "            class_metrics[cls]['TP'] += 1  # True positive\n",
    "        elif cls in pred_set and cls not in gt_set:\n",
    "            class_metrics[cls]['FP'] += 1  # False positive\n",
    "        elif cls not in pred_set and cls in gt_set:\n",
    "            class_metrics[cls]['FN'] += 1  # False negative\n",
    "    \n",
    "    return class_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c90100",
   "metadata": {},
   "source": [
    "### Evaluate on original test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8df864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_precision_at_threshold(model, test_images_path, ground_truth_path, class_names, confidence_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Evaluate model precision at a specified confidence threshold.\n",
    "    \n",
    "    Args:\n",
    "        model: YOLO model to evaluate\n",
    "        test_images_path: Path to the directory containing test images\n",
    "        ground_truth_path: Path to the directory containing ground truth labels\n",
    "        class_names: List of class names\n",
    "        confidence_threshold: Confidence threshold for predictions (default: 0.25)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (class_precision, macro_precision) where:\n",
    "            - class_precision is a dictionary mapping class names to precision values\n",
    "            - macro_precision is the average precision across all classes\n",
    "    \"\"\"\n",
    "    # Create dictionaries to store metrics\n",
    "    overall_metrics = {name: {'TP': 0, 'FP': 0, 'FN': 0} for name in class_names}\n",
    "    \n",
    "    # Process each image in the test directory\n",
    "    for img_path in glob.glob(os.path.join(test_images_path, '*.jpg')) + glob.glob(os.path.join(test_images_path, '*.png')):\n",
    "        try:\n",
    "            # Load image\n",
    "            img = Image.open(img_path)\n",
    "            img_filename = os.path.basename(img_path)\n",
    "            base_name = os.path.splitext(img_filename)[0]\n",
    "            \n",
    "            # Get corresponding label file path\n",
    "            label_path = os.path.join(ground_truth_path, base_name + '.txt')\n",
    "            \n",
    "            # Load ground truth data\n",
    "            gt_objects = read_label_file(label_path)\n",
    "            gt_classes = [obj['class_name'] for obj in gt_objects]\n",
    "            \n",
    "            # Run inference\n",
    "            prediction = model(img)\n",
    "            \n",
    "            # Process predictions\n",
    "            predicted_classes = []\n",
    "            \n",
    "            if len(prediction) > 0 and hasattr(prediction[0], 'boxes'):\n",
    "                boxes = prediction[0].boxes\n",
    "                \n",
    "                # Extract class information from predictions\n",
    "                for box in boxes:\n",
    "                    # Get the confidence score\n",
    "                    conf = float(box.conf)\n",
    "                    # Get the class id and name\n",
    "                    cls_id = int(box.cls)\n",
    "                    cls_name = prediction[0].names[cls_id]\n",
    "                    \n",
    "                    # Only include predictions above the specified confidence threshold\n",
    "                    if conf > confidence_threshold:\n",
    "                        predicted_classes.append(cls_name)\n",
    "            \n",
    "            # Compute class-based accuracy for this image\n",
    "            image_metrics = compute_class_accuracy(predicted_classes, gt_classes)\n",
    "            \n",
    "            # Update overall metrics\n",
    "            for cls in class_names:\n",
    "                overall_metrics[cls]['TP'] += image_metrics[cls]['TP']\n",
    "                overall_metrics[cls]['FP'] += image_metrics[cls]['FP']\n",
    "                overall_metrics[cls]['FN'] += image_metrics[cls]['FN']\n",
    "            \n",
    "        except Exception:\n",
    "            # Silently continue if there's an error with an image\n",
    "            pass\n",
    "    \n",
    "    # Calculate precision for each class\n",
    "    class_precision = {}\n",
    "    for cls in class_names:\n",
    "        tp = overall_metrics[cls]['TP']\n",
    "        fp = overall_metrics[cls]['FP']\n",
    "        \n",
    "        # Calculate precision\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        class_precision[cls] = precision\n",
    "    \n",
    "    # Calculate macro-averaged precision\n",
    "    macro_precision = sum(class_precision.values()) / len(class_precision)\n",
    "    \n",
    "    return class_precision, macro_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d8c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "precision = evaluate_precision_at_threshold(model, test_data_path, test_labels_path, class_names, confidence_threshold=0.25)\n",
    "print(\"\\nPrecision at 0.25 confidence threshold:\")\n",
    "print(\"{:<15} {:<10}\".format(\"Class\", \"Precision\"))\n",
    "for cls, prec in precision[0].items():\n",
    "    print(\"{:<15} {:<10.2f}\".format(cls, prec))\n",
    "print(\"\\nMacro-averaged precision:\")\n",
    "print(f\"{precision[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73470f",
   "metadata": {},
   "source": [
    "## Evaluate precision at different confidence thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3090c5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define confidence thresholds to evaluate\n",
    "confidence_thresholds = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "\n",
    "# Dictionary to store results\n",
    "threshold_results = {}\n",
    "\n",
    "# Evaluate precision for each confidence threshold\n",
    "for threshold in tqdm(confidence_thresholds, desc=\"Evaluating thresholds\"):\n",
    "    class_precision, macro_precision = evaluate_precision_at_threshold(\n",
    "        model, test_data_path, test_labels_path, class_names, confidence_threshold=threshold\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    threshold_results[threshold] = {\n",
    "        'class_precision': class_precision,\n",
    "        'macro_precision': macro_precision\n",
    "    }\n",
    "    \n",
    "    # Print current results\n",
    "    print(f\"\\nPrecision at {threshold:.2f} confidence threshold:\")\n",
    "    print(f\"Macro-averaged precision: {macro_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7449e515",
   "metadata": {},
   "source": [
    "## Visualize relationship between confidence threshold and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d07c03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract threshold and precision values for plotting\n",
    "thresholds = list(threshold_results.keys())\n",
    "macro_precisions = [result['macro_precision'] for result in threshold_results.values()]\n",
    "\n",
    "# Set up the figure for plotting\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Overall macro-precision vs confidence threshold\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(thresholds, macro_precisions, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Confidence Threshold')\n",
    "plt.ylabel('Macro-averaged Precision')\n",
    "plt.title('Relationship Between Confidence Threshold and Precision')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(thresholds)\n",
    "plt.ylim([0, 1.05])\n",
    "\n",
    "# Annotate points with precision values\n",
    "for i, precision in enumerate(macro_precisions):\n",
    "    plt.annotate(f\"{precision:.4f}\", \n",
    "                 (thresholds[i], precision),\n",
    "                 textcoords=\"offset points\", \n",
    "                 xytext=(0,10), \n",
    "                 ha='center')\n",
    "\n",
    "# Plot 2: Class-wise precision vs confidence threshold\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Extract class-wise precision for each threshold\n",
    "class_precisions = {}\n",
    "for cls in class_names:\n",
    "    class_precisions[cls] = [result['class_precision'][cls] for result in threshold_results.values()]\n",
    "    plt.plot(thresholds, class_precisions[cls], 'o-', linewidth=2, label=cls)\n",
    "\n",
    "plt.xlabel('Confidence Threshold')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Class-wise Precision vs Confidence Threshold')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(thresholds)\n",
    "plt.ylim([0, 1.05])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a table showing the precision values for each class and threshold\n",
    "print(\"\\nPrecision Values by Class and Confidence Threshold:\")\n",
    "print(\"{:<12} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "    \"Class\", *[f\"Conf {t:.1f}\" for t in thresholds]))\n",
    "print(\"-\" * 62)\n",
    "\n",
    "for cls in class_names:\n",
    "    print(\"{:<12} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\".format(\n",
    "        cls, *class_precisions[cls]))\n",
    "\n",
    "print(\"-\" * 62)\n",
    "print(\"{:<12} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\".format(\n",
    "    \"Macro-avg\", *macro_precisions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640d023a",
   "metadata": {},
   "source": [
    "## Testing augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8092eb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for augmented test images\n",
    "augmented_test_path = '/Users/jaydenma/Documents/mathematical image analysis/mia final project/Cars Detection/test/images_augmented'\n",
    "\n",
    "# Create lists to store results\n",
    "augmented_results = []\n",
    "augmented_total_images = 0\n",
    "augmented_overall_metrics = {name: {'TP': 0, 'FP': 0, 'FN': 0} for name in class_names}\n",
    "\n",
    "# Get mapping from augmented filename to original filename for ground truth lookup\n",
    "def get_original_filename(aug_filename):\n",
    "    # Augmented files typically have format: original_name_filter.ext\n",
    "    parts = os.path.splitext(aug_filename)[0].split('_')\n",
    "    # Remove the filter suffix to get original name\n",
    "    original_name = '_'.join(parts[:-1]) if len(parts) > 1 else parts[0]\n",
    "    return original_name\n",
    "\n",
    "# Process each image in the augmented test directory\n",
    "for img_path in tqdm(glob.glob(os.path.join(augmented_test_path, '*.jpg')) + \n",
    "                     glob.glob(os.path.join(augmented_test_path, '*.png'))):\n",
    "    try:\n",
    "        # Load image\n",
    "        img = Image.open(img_path)\n",
    "        img_filename = os.path.basename(img_path)\n",
    "        \n",
    "        # Get original filename to find corresponding label\n",
    "        original_base = get_original_filename(img_filename)\n",
    "        label_path = os.path.join(ground_truth_path, original_base + '.txt')\n",
    "        \n",
    "        # Load ground truth data\n",
    "        gt_objects = read_label_file(label_path)\n",
    "        gt_classes = [obj['class_name'] for obj in gt_objects]\n",
    "        \n",
    "        # Run inference\n",
    "        prediction = model(img)\n",
    "        \n",
    "        # Process predictions\n",
    "        predicted_classes = []\n",
    "        detections = []\n",
    "        \n",
    "        if len(prediction) > 0 and hasattr(prediction[0], 'boxes'):\n",
    "            boxes = prediction[0].boxes\n",
    "            \n",
    "            # Extract class information from predictions\n",
    "            for i, box in enumerate(boxes):\n",
    "                # Get the confidence score\n",
    "                conf = float(box.conf)\n",
    "                # Get the class id and name\n",
    "                cls_id = int(box.cls)\n",
    "                cls_name = prediction[0].names[cls_id]\n",
    "                # Get coordinates\n",
    "                coords = box.xyxy.tolist()[0] if hasattr(box.xyxy, 'tolist') else box.xyxy[0].tolist()\n",
    "                \n",
    "                # Only include predictions with confidence > 0.25\n",
    "                if conf > 0.25:\n",
    "                    predicted_classes.append(cls_name)\n",
    "                    detections.append({\n",
    "                        'confidence': conf,\n",
    "                        'class_id': cls_id,\n",
    "                        'class_name': cls_name,\n",
    "                        'box': coords\n",
    "                    })\n",
    "        \n",
    "        # Compute class-based accuracy for this image\n",
    "        image_metrics = compute_class_accuracy(predicted_classes, gt_classes)\n",
    "        \n",
    "        # Update overall metrics\n",
    "        for cls in class_names:\n",
    "            augmented_overall_metrics[cls]['TP'] += image_metrics[cls]['TP']\n",
    "            augmented_overall_metrics[cls]['FP'] += image_metrics[cls]['FP']\n",
    "            augmented_overall_metrics[cls]['FN'] += image_metrics[cls]['FN']\n",
    "        \n",
    "        # Store the result for this image\n",
    "        augmented_results.append({\n",
    "            'image_path': img_path,\n",
    "            'ground_truth': gt_classes,\n",
    "            'predictions': predicted_classes,\n",
    "            'detections': detections,\n",
    "            'metrics': image_metrics\n",
    "        })\n",
    "        \n",
    "        augmented_total_images += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "# Calculate overall metrics for augmented images\n",
    "augmented_class_precision = {}\n",
    "augmented_class_recall = {}\n",
    "augmented_class_f1 = {}\n",
    "\n",
    "for cls in class_names:\n",
    "    tp = augmented_overall_metrics[cls]['TP']\n",
    "    fp = augmented_overall_metrics[cls]['FP']\n",
    "    fn = augmented_overall_metrics[cls]['FN']\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    augmented_class_precision[cls] = precision\n",
    "    augmented_class_recall[cls] = recall\n",
    "    augmented_class_f1[cls] = f1\n",
    "\n",
    "# Print metrics for augmented images\n",
    "print(f\"Processed {augmented_total_images} augmented test images\")\n",
    "print(\"\\nClass-based metrics for augmented images:\")\n",
    "print(\"{:<15} {:<10} {:<10} {:<10}\".format(\"Class\", \"Precision\", \"Recall\", \"F1 Score\"))\n",
    "for cls in class_names:\n",
    "    print(\"{:<15} {:<10.2f} {:<10.2f} {:<10.2f}\".format(cls, augmented_class_precision[cls], augmented_class_recall[cls], augmented_class_f1[cls]))\n",
    "\n",
    "# Calculate macro-averaged metrics for augmented images\n",
    "augmented_macro_precision = sum(augmented_class_precision.values()) / len(augmented_class_precision)\n",
    "augmented_macro_recall = sum(augmented_class_recall.values()) / len(augmented_class_recall)\n",
    "augmented_macro_f1 = sum(augmented_class_f1.values()) / len(augmented_class_f1)\n",
    "\n",
    "print(\"\\nMacro-averaged metrics for augmented images:\")\n",
    "print(f\"Precision: {augmented_macro_precision:.4f}\")\n",
    "print(f\"Recall: {augmented_macro_recall:.4f}\")\n",
    "print(f\"F1 Score: {augmented_macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df30519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results between original and augmented datasets\n",
    "if total_images > 0 and augmented_total_images > 0:\n",
    "    print(\"\\nPerformance Comparison - Original vs. Augmented:\")\n",
    "    print(\"{:<15} {:<15} {:<15} {:<15}\".format(\"Metric\", \"Original\", \"Augmented\", \"Difference\"))\n",
    "    \n",
    "    # Compare precision\n",
    "    precision_diff = augmented_macro_precision - macro_precision\n",
    "    print(\"{:<15} {:<15.4f} {:<15.4f} {:<+15.4f}\".format(\"Precision\", macro_precision, augmented_macro_precision, precision_diff))\n",
    "    \n",
    "    # Compare recall\n",
    "    recall_diff = augmented_macro_recall - macro_recall\n",
    "    print(\"{:<15} {:<15.4f} {:<15.4f} {:<+15.4f}\".format(\"Recall\", macro_recall, augmented_macro_recall, recall_diff))\n",
    "    \n",
    "    # Compare F1 Score\n",
    "    f1_diff = augmented_macro_f1 - macro_f1\n",
    "    print(\"{:<15} {:<15.4f} {:<15.4f} {:<+15.4f}\".format(\"F1 Score\", macro_f1, augmented_macro_f1, f1_diff))\n",
    "    \n",
    "    # Print per-class comparisons\n",
    "    print(\"\\nPer-class F1 Score Comparison:\")\n",
    "    print(\"{:<15} {:<15} {:<15} {:<15}\".format(\"Class\", \"Original\", \"Augmented\", \"Difference\"))\n",
    "    for cls in class_names:\n",
    "        orig_f1 = class_f1[cls]\n",
    "        aug_f1 = augmented_class_f1[cls]\n",
    "        diff = aug_f1 - orig_f1\n",
    "        print(\"{:<15} {:<15.4f} {:<15.4f} {:<+15.4f}\".format(cls, orig_f1, aug_f1, diff))\n",
    "    \n",
    "    # Calculate overall improvement\n",
    "    if macro_f1 > 0:\n",
    "        percent_improvement = (f1_diff / macro_f1) * 100 if macro_f1 > 0 else float('inf')\n",
    "        print(f\"\\nOverall F1 Score {'improvement' if f1_diff >= 0 else 'reduction'}: {abs(percent_improvement):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8960d0d4",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d9475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure and axis for plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Bar chart for F1 scores comparison\n",
    "x = np.arange(len(class_names))\n",
    "width = 0.35\n",
    "\n",
    "# Plot F1 scores for original and augmented datasets\n",
    "original_f1_values = [class_f1[cls] for cls in class_names]\n",
    "augmented_f1_values = [augmented_class_f1[cls] for cls in class_names]\n",
    "\n",
    "plt.bar(x - width/2, original_f1_values, width, label='Original')\n",
    "plt.bar(x + width/2, augmented_f1_values, width, label='Augmented')\n",
    "\n",
    "plt.xlabel('Vehicle Classes')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score Comparison: Original vs. Augmented')\n",
    "plt.xticks(x, class_names, rotation=45)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
